*** ALGORITHMS AND DATA STRUCTURES ***

[RUN TIME ANALYSIS]
- worst/best/average case run time.
- notations:
   - omega: minimum required time. lowerbound. no less than given time. best case.
   - big-o: maximum required time. upperbound. no more than given time. worst case.
   - theta: between upperbound and lowerbound. on an average to given time. average case.
- running time complexities: 
   - constant O(1)
   - logarithmic O(logn)
   - linear O(n)
   - linear logarithmic O(nlogn)
   - quadratic O(n^2)
   - cubic O(n^3)
   - exponential O(2^n)

[RECURSION]
* function calls itself.
* recursive case and base case.
* base condition stops recursion.
* can be transformed into iteration. vice versa.
* each recursive call has its own stack frame isolated from each other.
* tail recursion: call function at the end. no stack overflow.
* head recursion: call function at the beginning. push previous states to stack then pop them from stack. stack overflow risk.
* taylor recursion: store previous states in accumulator then pass to next call. memory efficient.
* applicable to problems that can be broken down into similar sub-problems.
* examples: sum, factorial, fibonacci, binary search, towers of hanoi, stack, tree, quick sort, merge sort, divide and conquer, dynamic programming, ...

[LINEAR SEARCH]
* unsorted array. one by one. O(n). n: number of array elements.

[BINARY SEARCH]
* sorted array. O(logn). tail recursion. n: number of array elements.
* divide array in half. if smaller than middle take left-hand otherwise right-hand then repeat.

[TOWERS OF HANOI]
* O(2^n). n: number of disks.
* three rods. many disks. one disk at a time. uppermost disk can move. smaller on larger.

[BACKTRACKING]
* constraint satisfaction problems. combinatorial optimization problems. depth-first search.
* incrementally build possible canditates to solution. abandon invalid canditates and backtrack. solution tree.
* faster than brute-force which tests all possible solutions.
* exponential running time. too slow.
* n-queen's, vertex coloring, sudoku, knight's tour, np-complete, np-hard ...
* meta-heuristics: approximate solutions to np problems. ant-colony optimization, genetic algorithms, simulated annealing

[N-QUEEN's PROBLEM]
* placing n queens on an nxn chess board such that no queen threatens any other.

[VERTEX COLORING PROBLEM]
* np-complete problem.
* coloring vertices of a graph such that no two adjacent vertices share same color.
* chromatic number: smallest number of colors needed.
* there may be more than one solution.
* bipartite graph, making schedule, radio frequency assignment, register allocation, map coloring ...
* four-color theorem.
* power-welsh algorithm.

[KNIGHT's TOUR PROBLEM]
* knight visits every square on a chessboard exactly once. hamiltonian-path problem.
* closed knight's tour: knight's ending point is same as the starting point. hamiltonian-cycle problem.
* Schwenk Theorem: for an mxn chessboard the closed knight tour problem is always feasible, unless:
  - m and n are both odds
  - m = 1, 2 or 4
  - m = 3 and n = 4, 6 or 8

[DYNAMIC PROGRAMMING]
* dynamic programming: dividing complex problems into simpler sub-programs. overlapping sub-problems. O(n).
* memoization: store previous calculations and fetch them if needed later. don't recalculate. O(1).
* divide and conquer: combining optimal solutions to non-overlapping sub-problems. merge/quick sort.

[FIBONACCI NUMBERS]
F(n) = F(n-1) + F(n-2)
F(0) = 0, F(1) = 1

[KNAPSACK PROBLEM]
* combinatorial optimization. resource allocation.
* set of N items, each has mass w and value v. total mass in knapsack shouldn't exceed W. make total value V large as possible.
* divisible knapsack problem: if there are fractions of items then use greedy approach.
  - sort items according to their values. O(n*logn).
  - start with most valuable item and take as much as possible. then try with next item. O(n).
  - overall time complexity: O(n*logn) + O(n) = O(n*logn)
* 0-1 knapsack problem: no fractions.
* S[i][w]: maximum cost of items that fit inside a knapsack of size weight w, choosing from the first i items.
* S[i][w] = Math.max(S[i-1][w]; vi + S[i-1][w-wi])
  - weights columns: 0-W
  - items rows: 0-N
  - knapsack with weight capacity x, number of items y, each item has a value, obtain maximum possible value without exceeding x.
  - value in last row-column
* O(n*W): pseudo-polynomial. running time; polynomial in numeric value, exponential in the length of the input.

[BUBBLE SORT]
* simplest sorting algorithm that works by repeatedly swapping the adjacent elements if they are in wrong order.
* worst time complexity: O(n*n). array is reverse sorted.
* best time complexity: O(n). array is already sorted.
* auxiliary space: O(1).

[SELECTION SORT]
* repeatedly find minimum element (considering ascending order) from unsorted part and putit at the beginning.
* two subarrays in a given array; subarray which is already sorted, subarray which is unsorted.
* time complexity: O(n^2).
* auxiliary space: O(1).

[INSERTION SORT]
* array is virtually split into sorted and unsorted part. values from unsorted part are picked and placed at correct position in sorted part.
* time complexity: O(n*2).
* auxiliary space: O(1).

[BUCKET SORT]
* mainly useful when input is uniformly distributed over a range of floating point numbers between 0 and 1.
   - create n empty buckets.
   - put array elements in different buckets.
   - sort individual buckets.
   - concatenate all buckets into array.
* time complexity: O(n).

[MERGE SORT]
* divide and conquer algorithm.
* divides input array into two halves, calls itself for two halves, and then merges two sorted halves.
* time complexity: O(nlogn).
* auxiliary space: O(n).

[QUICK SORT]
* divide and conquer algorithm.
* pick element as pivot and partition given array around picked pivot. 
* pick first, last, random or median element as pivot.
* time complexity: worst O(n^2), best O(nlogn).

[HEAP SORT]
*  first find maximum element and place itt at end. repeat same process for remaining elements.
*  binary heap is complete binary tree where items are stored in special order such that value in parent node is greater(or smaller) than values in its two children nodes. 
* time complexity: O(nlogn).

[COUNTING SORT]
* sorting technique based on keys between a specific range.
* count number of objects having distinct key values. then calculate position of each object in output sequence.
* time complexity: O(n+k).
* auxiliary space: O(n+k).

[RADIX SORT]
* do digit by digit sort starting from least significant digit to most significant digit. Radix sort uses counting sort as a subroutine to sort.

[SHELL SORT]
* variation of insertion sort that allows exchange of far items rather than moving elements only one position ahead.

[ARRAY]
* collection of elements with specific type in a contiguous memory locations.
* index, points to memory location of an element in the array.
* first index is 0.
* random access, non-resizable, non-dynamic. dynamic array is resizable.
* one or multi dimensional.
* two-dimensional is called matrice containing rows and columns. [r][c].
* three-dimensional array is array of matrices. [depth][r][c].
* operations: declare, instantiate, initialize, access, insert, traverse, search, delete. dynamic array; add, remove.
* traverse: visit each element once.
* O(1): get single item, add to last, remove from last.
* O(n): reconstruction, insertion by pos, remove by pos. n: number of elements.
* insert/remove by pos takes much time because of shifting.

[LINKED LIST]
* each element is called node. node contains, data and reference to next node. head and tail nodes.
* elements are stored in non-contiguous memory locations.
* reference to next node at the last node points to NULL.
* sequential access, resizable, dynamic. runtime memory allocation.
* operations on first elements are more efficient.
* elements with different sizes are possible.
* high memory usage because of references.
* navigating backwards is diffucult.
* O(1): insert at start, remove from start. 
* O(N): insert at last, remove by pos.
* applications: stack, queue.
* types: single, circular single, double, double circular.
   - single: reference to next node in last node points to null.
   - double: each node holds reference to next node and previous node.
   - circular: reference to next node in last node points to first node.
* operations: create, insert, traverse, search, delete node, delete linked list.

[STACK]
* abstract. interface.
* last-in first-out.
* operations: pop, push, peek.
* graph algorithms: depth-first search, euler-cycles, strongly connected components, binary search tree traversal, linked list search, ...
* call stack: special memory region. os-managed. limited-size. not resizable. return points and temporary variables for subroutines. faster.
* heap: special memory region. user-managed. unlimited-size. resizable. deallocation required otherwise memory leak. pointers. objects. slower.
* recursive algorithms can be transformed into simple method with stacks. too many function calls results in stack overflow.
* O(1) running time; push, pop.

[QUEUE]
* abstract. interface.
* first-in first-out.
* operations: enqueue, dequeue, peek.
* graph algorithms: breadth-first search, graph reversal, threading, asynchronous data transfer,  stochastic models, ...
* head:first-node, tail:last-node.

[BINARY TREE]
* tree whose elements have at most 2 children is called binary tree. hierarchical data structure. array, linked-list, stack, queue are linear data structures.
* each element (parent node) in binary tree can have only 2 subnodes(children nodes) named as left and right child. top most node is root node.
* node with no children is called leaf.
* array: binary search fast O(logN), insert/remove slow O(N).
* linked list: binary search slow O(N), insert/remove fast O(1).
* binary search tree: binary search/insert/remove fast O(logN). (balanced tree)
* tree: root node + nodes + edges (connection between nodes)
* sorted keys.
* single path from root to any other node.
* root node - parent node - child node
   - maximum two children nodes per node.
   - left child is smaller than parent. 
   - right child is greater than parent.
   - leftmost node is smallest.
   - rightmost node is largest.
* balanced tree: left children = right children approximately.
* predecessor: largest in left subtree.
* successor: smallest in right subtree.
* height of tree: 
   - number of layers. 
   - h layer has 2^(h-1) nodes.
   - minimum height: h = logn
   - if tree is balanced, running time is logarithmic.
   - if tree is unbalanced running time is linear.
* insert: greater than root right, smaller than root left. O(logN) - O(N)
* search: greater than root right, smaller than root left. O(logN) - O(N)
* delete: O(logN) - O(N)
   - leaf node: simply remove.
   - single child node: update references.
   - two children node: predecessor/successor/root node swapping.
* soft delete: node not removed actually, rather marked as removed. not efficient. O(logN) + O(1)
* traversal: 
   - in-order: left subtree + root node + right subtree.
   - pre-order: root node + left subtree + right subtree.
   - post-order: left subtree + right subtree + root node.
* applications: hierarchical data. os file system, chess, tic-tac-toe, machine learning.

[PRIORITY QUEUE]
* abstract. priority.
* item with highest priority served first.
* implemented with heap, self-balancing tree.
* operations: insertWithPriority, getHighestPriority, peek
* sorting: sequence of decreasing priorities. tree-sort, heap-sort.

[HEAP]
* binary tree. priority queue. naturally balanced. dijkstra and prims algorithms.
* max heap: keys of parent nodes are greater. root maximum.
* min heap: keys of parent nodes are smaller. root minimum.
* parent node i: left node 2i+1: right node 2i+2
    0
 1     2
3 4   5 6
   ...
* construction O(N), reconstruction O(N) + O(logN), delete O(1) + O(logN)
* delete: last item to removed item.
* binomial heap: merging two heaps.
* fibonacci heap:  dijkstra's shortest path, prim's spanning tree, ...

[HEAP SORT]
* comparision based.
* worst-case O(NlogN). slower than quicksort. insert O(1). merge O(logN).
* additional memory not needed. in-place algorithm. insert O(1). merge O(1).
* swap root + maintain heap.
* memory complexity O(N), find min/max O(1), insert O(1) + O(log2(N)), remove O(1) + O(logN)

[ASSOCIATIVE ARRAY]
* abstract. key-value. unique keys. 
* implementation with binary search tree, hastable.
* operations: add, remove, update, search value by key.
* applications: database, word occurence count, storing data, lookup tables, rabin-karp algorithm, ...

[HASH TABLE/DICTIONARY]
* hash function
   - distribution of keys uniformly into buckets.
   - key space -> buckets (array slots).
   - certain key to any type -> random array index.
   - n: number of keys
   - m: number of buckets (size of array should be prime for uniform distribution for indexes)
   - h(x): k -> m (key to array index)
   - h(x) = n % m (integers)
   - h(x): add up and modulo ascii values for characters. (strings)
   - LF = n/m : load factor (empty ht: 0: memory waste, full ht: 1: slower)
* collision: different keys to same bucket.
   - chaining: store keys in same bucket, connect with linked list.
   - open addressing: generate new index for item. better than chaining.
      .linear probing: try next slot until empty one found.
      .quadratic probing: try 1/2/4/8/... slots far away.
      .rehashing: hash result again.
* space O(n), search O(1)/O(n), insert O(1)/O(n), delete O(1)/O(n)
* dynamic resizing: find optimal LF.
   - space-time tradeoff: LF exceeds threshold then resize.
   - java: if LF > 0.75 then resize automatically.
   - python: 2/3 ~ 0.66
   - resizing O(n)
* applications of hashing: index generation, crypthography, file comparision, password verification, blockchain, ...

[GRAPH THEORY]
* mathematical structures to model pairwise relations between given objects.
* G(V, E): graph = vertice + edge (vertice = node)
* types: directed, undirected
* representation models: adjaceny matrixes, edge list representation
* adjacency matrix
   - A(i, j) = 1 (nodes i, j are directly connected)
   - A(i, j) = 0 (nodes i, j are not directly connected)
   - A(i, i) = 0 (nodes are not connected to themselves)
* edge list representation
   - vertex class (vertex_name, visited, neighbors)
* applications: shortest path algorithm, graph traversing, spanning trees, maximum flow problem, ...

[BREADTH FIRST SEARCH]
* graph traversal algorithm.
* visit every node only once. visit all nodes row-by-row.
* queue, dequeue, visit children, repeat
* running time complexity: O(V+E)
* memory complexity bad. lots of references. O(N).
* first-in first out.
* applications: dijsktra, machine learning, edmonds-karp maximum flow, cheyen garbage collection, reference detection, serialization/deserialization, web-crawler, network topology, barabasi popular websites, ...

[DEPTH FIRST SEARCH]
* graph traversal algorithm. layer-by-layer algorithm.
* running time complexity: O(V+E) (vertices + edges)
* memory complexity better than bfs. O(logN).
* maze solving.
* explore as far as possible along each branch before backtracing.
* furthest vertex before visiting neighbour.
* applications: topological ordering, kosaraju strongly connected components algorithm, detecting cycles (DAG graph or not), generate/solve maze, ...