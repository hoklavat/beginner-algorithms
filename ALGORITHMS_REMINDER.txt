****ALGORITHMS****

[RECURSION]
* function call inside same function.
* can be transformed into iteration. vice versa.
* each recursive call has its own stack frame isolated from each other.
* tail recursion: call function at the end. no stack overflow.
* head recursion: call function at the beginning. push previous states to stack then pop them from stack. stack overflow risk.
* taylor recursion: store previous states in accumulator then pass to next call. memory efficient.
* sum, factorial, fibonacci, binary search, towers of hanoi, ...

[LINEAR SEARCH]
* unsorted array. one by one. O(N) running time. N array elements.

[BINARY SEARCH]
* sorted array. O(logN) running time. tail recursion. N array elements.
* divide array in half. if smaller than middle take left-hand otherwise right-hand then repeat.

[TOWERS OF HANOI]
* O(2^N) running time. N number of disks.
* three rods. many disks. one disk at a time. uppermost disk can move. smaller on larger.

[BACKTRACKING]
* constraint satisfaction problems. combinatorial optimization problems. depth-first search.
* incrementally build possible canditates to solution. abandon invalid canditates and backtrack. solution tree.
* faster than brute-force which tests all possible solutions.
* exponential running time. too slow.
* n-queen's, vertex coloring, sudoku, knight's tour, np-complete, np-hard ...
* meta-heuristics: approximate solutions to np problems. ant-colony optimization, genetic algorithms, simulated annealing

[N-QUEEN's PROBLEM]
* placing N queens on an NxN chess board such that no queen threatens any other.

[VERTEX COLORING PROBLEM]
* np-complete problem.
* coloring vertices of a graph such that no two adjacent vertices share same color.
* chromatic number: smallest number of colors needed.
* there may be more than one solution.
* bipartite graph, making schedule, radio frequency assignment, register allocation, map coloring ...
* four-color theorem.
* power-welsh algorithm.

[KNIGHT's TOUR PROBLEM]
* knight visits every square on a chessboard exactly once. hamiltonian-path problem.
* closed knight's tour: knight's ending point is same as the starting point. hamiltonian-cycle problem.
* Schwenk Theorem: for an mxn chessboard the closed knight tour problem is always feasible, unless:
  - m and n are both odds
  - m = 1, 2 or 4
  - m = 3 and n = 4, 6 or 8

[DYNAMIC PROGRAMMING]
* dynamic programming: dividing complex problems into simpler sub-programs. overlapping sub-problems. O(N) linear time complexity.
* memoization: store previous calculations and fetch them if needed later. don't recalculate. O(1) time complexity.
* divide and conquer: combining optimal solutions to non-overlapping sub-problems. merge/quick sort.

[FIBONACCI NUMBERS]
F(N) = F(N-1) + F(N-2)
F(0) = 0, F(1) = 1

[KNAPSACK PROBLEM]
* combinatorial optimization. resource allocation.
* set of N items, each has mass w and value v. total mass in knapsack shouldn't exceed W. make total value V large as possible.
* divisible knapsack problem: if there are fractions of items then greedy approach.
  - sort items according to their values. O(N*logN) time complexity.
  - start with most valuable item and take as much as possible. then try with next item. O(N) time complexity.
  - overall time complexity: O(N*logN) + O(N) = O(N*logN)
* 0-1 knapsack problem: no fractions.
* S[i][w]: maximum cost of items that fit inside a knapsack of size weight w, choosing from the first i items.
* S[i][w] = Math.max(S[i-1][w]; vi + S[i-1][w-wi])
  - weights columns: 0-W
  - items rows: 0-N
  - knapsack with weight capacity x, number of items y, each item has a value, obtain maximum possible value without exceeding x.
  - value in last row-column
* O(n*W) running time. pseudo-polynomial. running time; polynomial in numeric value, exponential in the length of the input.

[ARRAY]
* index-value. 
* first index 0.
* random access.
* two-dimensional, matrix, [row][column].
* non-resizable, non-dynamic.
* dynamic array: resizable.
* O(1) constant running time; get single item, add to last, remove from last.
* O(N) linear running time; reconstruction, insertion by pos, remove by pos.
* insert/remove by pos takes much time because of shifting.
* operations: add, insert, remove

[LINKED LIST]
* node: data + next-node-reference.
* next-node-reference at last node points to NULL.
* no random access. sequential access.
* dynamic. runtime memory allocation.
* efficient for first elements.
* elements with different sizes.
* stack, queue implementation.
* high memory due to references.
* diffucult to navigate backwards.
* O(1) running time; insert at start, remove from start. 
* O(N) running time; insert at last, remove by pos.
* operations: insertAtStart, insertAtEnd, removeStart, remove.

[STACK]
* abstract. interface.
* last-in first-out.
* operations: pop, push, peek.
* graph algorithms: depth-first search/euler-cycles/strongly connected components, binary search tree traversal, linked list search, ...
* call stack: special memory region. os-managed. limited-size. not resizable. return points and temporary variables for subroutines. faster.
* heap: special memory region. user-managed. unlimited-size. resizable. deallocation required otherwise memory leak. pointers. objects. slower.
* recursive algorithms can be transformed into simple method with stacks. too many function calls results in stack overflow.
* O(1) running time; push, pop.

[QUEUE]
* abstract. interface.
* first-in first-out.
* operations: enqueue, dequeue, peek.
* graph algorithms: breadth first search graph reversal, threading, asynchronous data transfer,  stochastic models.
* head:first-node, tail:last-node.

[BINARY SEARCH TREE]
* array: binary search fast O(logN), insert/remove slow O(N).
* linked list: binary search slow O(N), insert/remove fast O(1).
* binary search tree: binary search/insert/remove fast O(logN). (balanced tree)
* tree: root node + nodes + edges (connection between nodes)
* sorted keys.
* single path from root to any other node.
* root node - parent node - child node
   - maximum two children nodes per node.
   - left child is smaller than parent. 
   - right child is greater than parent.
   - leftmost node is smallest.
   - rightmost node is largest.
* leaf node has no children.
* balanced tree: left children = right children approximately.
* predecessor: largest in left subtree.
* successor: smallest in right subtree.
* height of tree: 
   - number of layers. 
   - h layer has 2^(h-1) nodes.
   - minimum height: h = logn
   - if tree is balanced, running time is logarithmic.
   - if tree is unbalanced running time is linear.
* insert: greater than root right, smaller than root left. O(logN) - O(N)
* search: greater than root right, smaller than root left. O(logN) - O(N)
* delete: O(logN) - O(N)
   - leaf node: simply remove.
   - single child node: update references.
   - two children node: predecessor/successor/root node swapping.
* soft delete: node not removed actually, rather marked as removed. not efficient. O(logN) + O(1)
* traversal: 
   - in-order: left subtree + root node + right subtree.
   - pre-order: root node + left subtree + right subtree.
   - post-order: left subtree + right subtree + root node.
* applications: hierarchical data. os file system, chess, tic-tac-toe, machine learning.

[PRIORITY QUEUE]
* abstract. priority.
* item with highest priority served first.
* implemented with heap, self-balancing tree.
* operations: insertWithPriority, getHighestPriority, peek
* sorting: sequence of decreasing priorities. tree-sort, heap-sort.

[HEAP]
* binary tree. priority queue. naturally balanced. dijkstra and prims algorithms.
* max heap: keys of parent nodes are greater. root maximum.
* min heap: keys of parent nodes are smaller. root minimum.
* parent node i: left node 2i+1: right node 2i+2
    0
 1     2
3 4   5 6
   ...
* construction O(N), reconstruction O(N) + O(logN), delete O(1) + O(logN)
* delete: last item to removed item.
* binomial heap: merging two heaps.
* fibonacci heap:  dijkstra's shortest path, prim's spanning tree, ...

[HEAP SORT]
* comparision based.
* worst-case O(NlogN). slower than quicksort. insert O(1). merge O(logN).
* additional memory not needed. in-place algorithm. insert O(1). merge O(1).
* swap root + maintain heap.
* memory complexity O(N), find min/max O(1), insert O(1) + O(log2(N)), remove O(1) + O(logN)

[ASSOCIATIVE ARRAY]
* abstract. key-value. unique keys. 
* implementation with binary search tree, hastable.
* operations: add, remove, update, search value by key.
* applications: database, word occurence count, storing data, lookup tables, rabin-karp algorithm, ...

[HASH TABLE/DICTIONARY]
* hash function
   - distribution of keys uniformly into buckets.
   - key space -> buckets (array slots).
   - certain key to any type -> random array index.
   - n: number of keys
   - m: number of buckets (size of array should be prime for uniform distribution for indexes)
   - h(x): k -> m (key to array index)
   - h(x) = n % m (integers)
   - h(x): add up and modulo ascii values for characters. (strings)
   - LF = n/m : load factor (empty ht: 0: memory waste, full ht: 1: slower)
* collision: different keys to same bucket.
   - chaining: store keys in same bucket, connect with linked list.
   - open addressing: generate new index for item. better than chaining.
      .linear probing: try next slot until empty one found.
      .quadratic probing: try 1/2/4/8/... slots far away.
      .rehashing: hash result again.
* space O(n), search O(1)/O(n), insert O(1)/O(n), delete O(1)/O(n)
* dynamic resizing: find optimal LF.
   - space-time tradeoff: LF exceeds threshold then resize.
   - java: if LF > 0.75 then resize automatically.
   - python: 2/3 ~ 0.66
   - resizing O(n)