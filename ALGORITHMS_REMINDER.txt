*** ALGORITHMS AND DATA STRUCTURES ***

[RUN TIME ANALYSIS]
* asymptotic analysis: how time or space taken by algorithm increases with input size n.
* worst/best/average case run time.
* asymptotic notations:
   - omega: minimum required time. lowerbound. no less than given time. best case. o().
   - big-o: maximum required time. upperbound. no more than given time. worst case. O().
   - theta: between upperbound and lowerbound. on average to given time. average case. T().
* common types of running time complexities for big-o:
   - O(1): constant. none loop.
   - O(logn): logarithmic. loop variable is divided/multiplied by constant.
   - O(loglogn): loop variable is decreased/increased exponentially by constant.
   - O(n): linear. constant loop.
   - O(m+n): consecutive loops.
   - O(nlogn): linear logarithmic.
   - O(n^x): inner loops.
   - O(n^2): quadratic.
   - O(n^3): cubic.
   - O(2^n): exponential.
* constant multiplication: if f(n) is O(g(n)) then a*f(n) is also O(g(n)). 
* transitive propoerty: if f(n) is O(g(n)) and g(n) is O(h(n)) then f(n) = O(h(n)). 
* reflexive property: If f(n) is given then f(n) is O(f(n)).
* symmetry: if f(n) is T(g(n)) then g(n) is T(f(n)). only valid for theta.
* transpose symmetry: if f(n) is O(g(n)) then g(n) is O(f(n)). only valid for big-o and omega.
* if f(n)=O(g(n)) and f(n)=o(g(n)) then f(n)=T(g(n)).
* if f(n)=O(g(n)) and d(n)=O(e(n)) then f(n)+d(n)=O(max(g(n),e(n))).
* if f(n)=O(g(n)) and d(n)=O(e(n)) then f(n)*d(n)=O(g(n)*e(n)).
* polynomial time algorithm: algorithm whose time complexity is only based on number of elements in array (not value).
* pseudo-polynomial algorithm: algorithm whose worst case time complexity depends on numeric value of input (not number of inputs).

[RECURSION]
* recursive function: function calls itself until base case is reached.
* commonly used in problems that can be broken down into similar sub-problems.
* applications: sum, factorial, fibonacci, binary search, towers of hanoi, stack, tree, quick sort, merge sort, divide and conquer, dynamic programming, ...
* recursion is stopped when base condition occurs.
* recursion can be transformed into iteration. 
* iteration can be transformed into recursion.
* each recursive call has its own stack frame isolated from each other.
* tail recursion: function calls itself at end. it prevents stack overflow.
* head recursion: function calls itself at beginning. it can cause stack overflow, because it pushes previous states to stack, then pops them from stack.
* taylor recursion: function stores previous states in accumulator then passes to next call. it provides memory efficieny.

[FACTORIAL]
* n! = 1 * 2 * 3 * ... * n
* factorial of number n can be calculated with recursion.

[FIBONACCI]
* F(n) = F(n-1) + F(n-2) (F(0)=0, F(1)=1)
* nth fibonacci number can be calculated with recursion.

[LINEAR SEARCH]
* start from beginning compare each element with searched value until end. 
* used for unsorted arrays. 
* running time complexity: O(n). (n: number of elements)

[BINARY SEARCH]
* divide sorted array in half. if searched value smaller than middle take left-hand otherwise right-hand then repeat same process for new partition.
* used for sorted arrays.
* uses tail recursion.
* running time complexity: O(logn). (n: number of elements)

[HANOI TOWERS]
* there are three rods and many disks with different sizes. smaller disk is located on larger disk. 
* establish same settlement on last rod by moving uppermost disk on first two rods. only one disk can be moved at time.
* minimal number of moves required to solve a Tower of Hanoi puzzle is 2^n - 1, where n is number of disks.
* running time complexity: O(2^n). (n: number of disks)

[BACKTRACKING]
* build possible canditates to solution incrementally. abandon invalid canditates then backtrack. 
* trying every possible configuration and each configuration is tried only once.
* solution tree is created.
* it is much faster than brute-force method which tests all possible solutions.
* applications: n-queen's, vertex coloring, sudoku, knight's tour, np-complete, np-hard, constraint satisfaction, combinatorial optimization, depth-first search,
 meta-heuristics, ...
* meta-heuristics: approximate solutions to np problems. ant-colony optimization, genetic algorithms, simulated annealing ...
* running time complexity: O(2^n). (n: number of elements)

[N-QUEEN's PROBLEM]
* place n queens on nxn chess board such that no queen threatens any other.

[VERTEX COLORING PROBLEM]
* color vertices of graph such that no two adjacent vertices share same color.
* chromatic number is minimum number of colors required.
* there may be more than one solution.
* applications: np-complete problem, four-color theorem, power-welsh algorithm, bipartite graph, making schedule, radio frequency assignment, register allocation,
 map coloring ...

[KNIGHT's TOUR PROBLEM]
* given nxn chess board with knight placed on first square of empty board. chess knight must visit each square exactly once without violating chess rules.
* knight visits every square on chessboard exactly once. (hamiltonian-path problem)
* beginning and ending squares are same. (hamiltonian-cycle problem, closed knight's tour)
* schwenk theorem: for mxn chessboard closed knight tour problem is always feasible, unless (m and n are both odds) or (m = 1/2/4) or (m = 3 and n = 4/6/8).

[DYNAMIC PROGRAMMING]
* dynamic programming: divide complex problems into simpler sub-programs then overlap sub-problems.
* two main properties of problem that suggests that given problem can be solved using dynamic programming are overlapping subproblems, optimal substructure.
* memoization (top down): first look into lookup table, if not exist calculate value and put result in lookup table for later use. filled on demand.
* tabulation (bottom up): builds table in bottom up fashion and returns last entry from table. filled one by one.
* optimal substructure: optimal solution of given problem can be obtained by using optimal solutions of its subproblems.
* running time complexity: O(n). 

* memoization: store previous calculations in table, then retrieve needed one from table rather than calculating again. 
* running time complexity: O(1).

* divide and conquer: combining optimal solutions to non-overlapping sub-problems. 
* applications: merge sort, quick sort.

[KNAPSACK PROBLEM]
* there are set of N items where each item has mass w and value v. total mass in knapsack shouldn't exceed limit W. make total value of items V as large as possible.
* combinatorial optimization. resource allocation.
* divisible knapsack problem: 
  - if there are fractions of items, greedy approach is used.
  - sort items according to their values.
  - start with most valuable item and take as much as possible.
  - then try with next item.
  - running time complexity: O(n*logn)
* 0-1 knapsack problem: there are no fractions. weights are integers.
  - S[i][w] = Max(S[i-1][w], vi + S[i-1][w-wi]) (maximum cost of items that fit inside knapsack of size weight w, choosing from first i items.)
  - columns represents possible total weights starting from 0 to W, incremented by one.
  - rows represents items starting from 0 to N..
  - value in last row-column gives result.
  - running time complexity: O(n*W).

[BUBBLE SORT]
* simplest sorting algorithm that works by repeatedly swapping adjacent elements if they are in wrong order.
* worst running time complexity: O(n^2). array is reverse sorted.
* best running time complexity: O(n). array is already sorted.

[SELECTION SORT]
* there are two subarrays in given array; subarray which is already sorted, subarray which is unsorted.
* repeatedly find minimum element (considering ascending order) from unsorted part and put it at beginning of sorted part.
* running time complexity: O(n^2).

[INSERTION SORT]
* array is virtually split into sorted and unsorted parts. values from unsorted part are picked and placed at correct position in sorted part.
* running time complexity: O(n*2).

[BUCKET SORT]
* mainly useful when input is uniformly distributed over range of floating point numbers between 0 and 1.
   - create n empty buckets.
   - put array elements in different buckets.
   - sort individual buckets.
   - concatenate all buckets into array.
* running time complexity: O(n).

[MERGE SORT]
* divide and conquer algorithm.
* divide input array into two halves, recursively call self for two halves, then merge two sorted halves.
* time complexity: O(nlogn).

[QUICK SORT]
* divide and conquer algorithm.
* pick element(first, last, median or random) as pivot, then partition given array around picked pivot. 
* worst running time complexity: O(n^2).
* best running time complexity: O(nlogn).

[HEAP SORT]
* find maximum element and place it at end, then repeat same process for remaining elements.
* slower than quicksort. 
* additional memory is not needed.
* running time complexity: worst case O(nlogn), insert O(1), merge O(logN).

[COUNTING SORT]
* sorting technique based on keys between specific range.
* count number of objects having distinct key values, then calculate position of each object in output sequence.
* running time complexity: O(n+k).
* auxiliary space: O(n+k).

[RADIX SORT]
* digit by digit sort starting from least significant digit to most significant digit. 
* radix sort uses counting sort as subroutine to sort.

[SHELL SORT]
* variation of insertion sort that allows exchange of far items rather than moving elements only one position ahead.

[ARRAY]
* collection of elements with specific type in contiguous memory locations.
* index points to memory location of element in array.
* index of first element is 0.
* random access: user can access any element directly without traversing other elements.
* fixed size: cannot be resized after declaration.
* non-dynamic: cannot add new element or remove existing element.
* dynamic array is resizable and dynamic. 
* dimension: can be multi dimensional.
* two-dimensional array is called matrice that contains rows and columns. [r][c].
* three-dimensional array is called array of matrices. [depth][r][c].
* operations: declare, instantiate, initialize, access, insert, traverse, search, delete. in dynamic array; add and remove operations are also possible.
* traverse: visit each element once.
* running time complexity: O(1) for get single item, add to last, remove from last. O(n) for reconstruction, insertion by pos, remove by pos where n is number of 
elements. insert/remove by pos takes much time because of shifting.

[LINKED LIST]
* linked list consists of nodes which contain data and reference to next node.
* nodes are stored in non-contiguous memory locations.
* head node is first node in list. 
* tail node is last node in list.
* reference to next node at last node points to NULL.
* sequential access: cannot access node without traversing on previous ones. 
* resizable: new node can be inserted.
* dynamic: memory can be allocated at runtime.
* operations are more efficient on first elements.
* elements having different sizes are possible.
* memory usage is high because of references.
* backward navigation is diffucult.
* applications: stack, queue.
* types: single, single circular, double, double circular.
   - single: reference to next node in tail node points to null.
   - double: each node holds reference to next node and previous node.
   - circular: reference to next node in tail node points to head node.
* operations: create, insert, traverse, search, delete node, delete linked list.
* running time complexity: O(1) for insert at start, remove from start, O(N) for insert at last, remove by pos.

[STACK]
* abstract interface.
* last-in first-out.
* operations:
   - push: put at top.
   - pop: get from top.
   - peek: what is at top.
* applications: graph algorithms, depth-first search, euler cycles, strongly connected components, binary search tree traversal, linked list search, ...
* call stack: operating system managed, limited and fixed sized, special memory region that is used for return points and temporary variables for subroutines.
* heap: user managed, unlimited and dynamic sized, special memory region that is used for pointers and objects. deallocation required otherwise memory leak. 
* heap is slower than stack.
* recursive algorithms can be transformed into simple method with stacks. 
* too many function call can result in stack overflow.
* running time complexity: O(1) for push, pop.

[QUEUE]
* abstract interface.
* first-in first-out.
* operations: 
   - enqueue: put at start node (head).
   - dequeue: get from end node (tail).
   - peek: what is at end.
* applications: graph algorithms, breadth-first search, graph reversal, threading, asynchronous data transfer,  stochastic models, ...

[BINARY TREE]
* hierarchical data structure, that organizes nodes where each node (parent) have at most 2 children (left and right child). 
* array, linked-list, stack, queue are linear data structures.
* node at top is root node.
* there is only one path from root node to any other node.
* node with no children is leaf.
* array: binary search fast O(logN), insert/remove slow O(N).
* linked list: binary search slow O(N), insert/remove fast O(1).
* binary search tree: binary search/insert/remove fast O(logN). (balanced tree)
* data inserted as sorted.
   - left child is smaller than parent. 
   - right child is greater than parent.
   - left-most node is smallest.
   - right-most node is largest.
* in balanced tree left children are equal to right children approximately.
* predecessor: largest in left subtree.
* successor: smallest in right subtree.
* structure of tree: 
   - number of layers. 
   - ith layer has 2^(i-1) nodes.
   - minimum height: h = logn
* running time complexity: 
   - if tree is balanced, running time complexity is logarithmic. 
   - if tree is unbalanced, running time complexity is linear.
   - insert: O(logN) - O(N).
   - search: O(logN) - O(N).
   - delete: O(logN) - O(N).
   - soft delete: O(logN) + O(1).
   - leaf node: simply remove.
   - single child node: update references.
   - two children node: predecessor/successor/root node swapping.
* soft delete: node isn't removed actually, rather it is marked as removed. not efficient. 
* traversal methods: if data to be inserted is greater than root traverse right, if smaller traverse left.
   - in-order: left subtree + root node + right subtree.
   - pre-order: root node + left subtree + right subtree.
   - post-order: left subtree + right subtree + root node.
* applications: hierarchical data, os file system, chess, tic-tac-toe, machine learning.

[PRIORITY QUEUE]
* abstract data structure.
* every item has priority associated with it. item with highest or lowest priority served first.
* implemented with heap or self-balancing tree.
* operations: insert, getHighestPriority, deleteHighestPriority, peek

[BINARY HEAP]
* binary heap is complete binary tree where items are stored in special order such that parent node key is greater/smaller than children node keys. 
* all levels are completely filled except possibly last level.
* minimum binary heap: root key must be minimum among all keys. parent node is smaller than children.
* maximum binary heap: root key must be maximum among all keys. parent node is greater than children.
* indexes: 
   - parent: i
   - left child: 2i+1
   - right child: 2i+2

* delete: last item placed into removed item.
* binomial heap: constructed by merging two heaps.
* fibonacci heap: used in dijkstra's shortest path, prim's spanning tree algorithms.
* running time complexity: construction O(N), reconstruction O(N) + O(logN), delete O(1) + O(logN).
* applications: priority queue, dijkstra's shortest path algorithm, prim's spanning tree algorithm.

[ASSOCIATIVE ARRAY]
* abstract data structure.
* also called as map or dictionary.
* data organized in key and value pairs.
* keys are unique. no duplicate keys.
* implementated with binary search tree or hashtable.
* operations: add, remove, update, search value by key.
* applications: database, word occurence count, storing data, lookup tables, rabin-karp algorithm, ...

[HASH TABLE] 
* hash function maps keys to some values.
* keys are distributed uniformly into buckets.
* buckets are also called as key space or array slots.
* size of array should be prime for uniform distribution of indexes.
* parameters of hash table:
   - number of keys: n
   - number of buckets: m
   - hash function: h(x) : k->m (key is assigned to bucket by formula.) 
   - for integer type: bucket corresponding to key is calculated by taking modulo. (h(x) = n%m)
   - for string type: add up and modulo ascii values for characters.
   - load factor: determines memory efficiency and speed optimization. LF = n/m (0: no keys causes memory waste, 1: full table causes slowing)
* collision occurs when different keys are assigned to same bucket. collision prevention methods are chaining, open addressing.
   - chaining: keys in same bucket are connected with linked list.
   - open addressing: new index is generated for item. it is better than chaining.
      .linear probing: try next slot until empty one found.
      .quadratic probing: try 1/2/4/8/... slots far away.
      .rehashing: hash result again.
* running time complexity: space O(n), search O(1)/O(n), insert O(1)/O(n), delete O(1)/O(n), resizing O(n).
* dynamic resizing: find optimal load factor.
   - space-time tradeoff: if LF exceeds threshold then resize.
   - java: if LF > 0.75 then resize automatically.
   - python: 2/3 ~ 0.66
* applications: index generation, crypthography, file comparision, password verification, blockchain, ...

[GRAPH]
* non-linear data structure consisting of nodes and edges which connect nodes with each other.
* types: directed, undirected
* node = vertice.
* edge represents connection between two nodes. 
* adjacency matrix (nx) or edge list(n) represents which nodes are connected to each other. n is number of vertices.
* adjacency matrix for undirected graph is symmetrical.
* for directed graph (u, v) is not same as (v, u) where v and u are nodes.
* mathematical structures to model pairwise relations between given objects.
* directed graph is called strongly connected if there is path from each vertex in graph to every other vertex.
* bipartite graph is graph whose vertices can be divided into two disjoint and independent sets U and V such that every edge connects vertex in U to one in V.
* adjacency matrix
   - A(i, j) = 1 (nodes i, j are directly connected)
   - A(i, j) = 0 (nodes i, j are not directly connected)
   - A(i, i) = 0 (nodes are not connected to themselves)
* edge list representation: vertex class contains vertex name, visited or not, neighbor nodes.
* applications: shortest path algorithm, graph traversing, spanning trees, maximum flow problem, ...

[BREADTH FIRST SEARCH]
* graph traversal algorithm. row-by-row traversal.
* queue, dequeue, visit children, repeat.
* running time complexity: O(V+E) (vertices + edges)
* memory complexity bad. lots of references. O(N).
* first-in first out.
* applications: dijsktra, machine learning, edmonds-karp maximum flow, cheyen garbage collection, reference detection, serialization/deserialization, web-crawler, 
network topology, barabasi popular websites, ...

[DEPTH FIRST SEARCH]
* graph traversal algorithm. layer-by-layer traversal.
* running time complexity: O(V+E) (vertices + edges)
* memory complexity better than bfs. O(logN).
* maze solving.
* explore as far as possible along each branch before backtracing.
* furthest vertex before visiting neighbour.
* applications: topological ordering, path finding, kosaraju strongly connected components algorithm, detecting cycles (DAG graph or not), generate/solve maze, ...
* topological sorting is mainly used for scheduling jobs from given dependencies among jobs.

[AVL TREE]
* self-balancing binary search tree where the difference between heights of left and right subtrees cannot be more than one for all nodes.
* tree rotation is operation that changes structure without interfering with order of elements on avl tree.
* height of an avl tree is always O(Logn) where n is the number of nodes in the tree.
* methods used to rebalance bst without violating bst property: left rotation, right rotation.
* there are the possible 4 arrangements for rotation operations; z is first unbalanced node, y is child of z, x is grandchild of z.
   - y is left child of z and x is left child of y (left-left case)
   - y is left child of z and x is right child of y (left-right case)
   - y is right child of z and x is right child of y (right-right case)
   - y is right child of z and x is left child of y (right-left case)

[TRIE]
* tree based data structure, used for efficient retrieval ofkey in large dataset of strings.
* key character acts as an index into the array children.

[TOPOLOGICAL SORTING]
* topological sorting for directed acyclic graph (DAG) is linear ordering of vertices such that for every directed edge from u to v, vertex u comes before v in
ordering. topological sorting for graph is not possible if graph is not DAG.
* directed acyclic graph (DAG) is directed graph with no directed cycles. it consists of vertices and edges (arcs), with each edge directed from one vertex to another 
such that there is no way to start at any vertex v and follow consistently-directed sequence of edges that eventually loops back to v again.

[DIJKSTRA SHORTEST PATH ALGORITHM]
* given graph and source vertex in graph, find shortest paths from source to all vertices in given graph.
* greedy graph algorithm.
* generate SPT (shortest path tree) with given source as root. maintain two sets, one set contains vertices included in shortest path tree, other set includes vertices
not yet included in shortest path tree. at every step of algorithm, we find vertex which is in other set (set of not yet included) and has minimum distance 
from source.
* running time complexity: O(VLogV). (V: number of vertices)

[BELLMAN-FORD SHORTEST PATH ALGORITHM]
* given graph and source vertex in graph, find shortest paths from source to all vertices in given graph. graph may contain negative weight edges.
* dynamic graph algorithm.
* running time complexity: O(VE). (V: number of vertices, E: number of edges)

[FLOYD WARSHALL SHORTEST PATH ALGORITHM]
* find shortest distances between every pair of vertices in given edge weighted directed graph.
* dynamic graph algorithm.
* running time complexity: O(V^3). (V: number of vertices)

[UNION FIND ALGORITHM]
* disjoint set data structure keeps track of set of elements partitioned into number of disjoint subsets.
* union-find algorithm performs two useful operations on disjoint set data structures:
   - find: determine which subset particular element is in.
   - union: join two subsets into single subset.
* union-find algorithm can be used to check whether undirected graph contains cycle or not.

[KRUKSAL MINIMUM SPANNNG TREE ALGORITHM]
* spanning tree of connected and undirected graph is subgraph that is tree and connects all vertices together.
* minimum spanning tree is spanning tree with weight less than or equal to weight of every other spanning tree.
* weight of aspanning tree is sum of weights given to each edge of spanning tree.
* minimum spanning tree has (V – 1) edges where V is the number of vertices in the given graph.
* greedy graph algorithm.

[PRIM MINIMUM SPANNING TREE ALGORITHM]
* maintain two sets of vertices. first set contains vertices already included in MST, other set contains vertices not yet included. considers all edges that 
connect two sets, and pick minimum weight edge from these edges.
* group of edges that connects two set of vertices in graph is called cut. find cut of two sets, pick minimum weight edge from cut and include this vertex to MST set.
* greedy graph algorithm.

[GREEDY ALGORITHM]
* algorithmic paradigm that builds up solution piece by piece, always choosing next piece that offers most obvious and immediate benefit. 
* greedy algorithms are used for optimization problems. 
* optimization problem can be solved using greedy if problem has following property: at every step, choice that looks best at moment can be made and optimal solution
 of complete problem can be gotten.
* examples: Kruskal Minimum Spanning Tree, Prim Minimum Spanning Tree, Dijkstra Shortest Pat, Huffman Coding.

[ACTIVITY SELECTION PROBLEM]
* n activities are given with their start and finish times. Select maximum number of activities that can be performed by single person, assuming that person can only
 work on single activity at time.
* greedy choice is to always pick next activity whose finish time is least among remaining activities and start time is more than or equal to finish time of previously
 selected activity.
* greedy algorithm.

[COIN CHANGE PROBLEM]
* find minimum number of coins needed to make change when there is infinite supply for each of denominations.
* take coins with greater value first, reduces total number of coins needed. start from largest possible denomination and keep adding denominations while remaining 
value is greater than 0.
* greedy algorithm.

[FRACTIONAL KNAPSACK PROBLEM]
* given weights and values of n items, put these items in knapsack of capacity W to get maximum total value in knapsack. in 0-1 knapsack problem it is not allowed to
 break items. in fractional knapsack items can be broke down for maximizing total value of knapsack.
* calculate ratio value/weight for each item and sort item on basis of this ratio. then take item with highest ratio and add them until next item cannot be added as
 whole and at end add next item as much as we can. 

[DIVIDE AND CONQUER ALGORITHM]
* divide: break given problem into subproblems of same type. 
* conquer: recursively solve these subproblems. 
* combine: appropriately combine the answers.
* examples: binary search, quick sort, merge sort, closest pair of points, strassen's algorithm, cooley-tukey fast fourier transform, karatsuba algorithm, ...

[NUMBER OF WAYS ALGORITHM]
* find number of different ways to express number as sum of specific numbers.

[HOUSE OF THIEF ALGORITHM]
* there are n houses build in line, each of which contains some value in it. thief is going to steal maximal value of these houses, but he can’t steal in two adjacent
 houses because owner of stolen houses will tell his two neighbours left and right side. what is the maximum stolen value?
* solution is to find maximum sum subsequence where no two selected elements are adjacent.
* if element is selected then next element cannot be selected. if element is not selected then next element can be selected.

[STRING CONVERSION]
* convert one string to another by deleting, inserting and replacing characters. find minimum number of edit operations.
* divide and conquer algorithm.

[LONGEST COMMON SUBSEQUENCE]
* given two strings, find longest subsequence which is common in both strings.

[LONGEST PALINDROMIC SUBSEQUENCE]
* find longest palindromic subsequence in given string.
* subsequence is sequence that can be derived from another string by deleting some or no elements without changing order of remaining elements.
* palindrom is string that reads same backward as well as forward. 

[LONGEST PALINDROMIC SUBSTRING]
* find longest palindromic substring in given string.
* substring is contiguous sequence of characters within string.

[MINIMUM COST TO REACH END]
* two dimensional matrix is given. accessing each cell have cost associated with it. started from (0, 0) go till (n-1, n-1) cell.
* only movement to right or down cell from current cell is possible.
* find minimum cost to reach end.

[NUMBER OF PATHS TO REACH END]
* two dimensional matrix is given. accessing each cell have cost associated with it. started from (0, 0) go till (n-1, n-1) cell.
* only movement to right or down cell from current cell is possible.
* find number of ways to reach end, by not exceeding total cost limit.

[MAXIMUM FLOW PROBLEM]
* maximum flow problems involve finding feasible flow through single-source, single-sink flow network that is maximum.
* figure out how much stuff can be pushed from source vertex to sink vertex.
* each edge has capacity. flow should not exceed this capacity for single path. f(e) < c(e). so maximum flow for single path is least capacity among all edges on this path.
* over all maximum flow is found by summing individual flows of paths.
* residual graph of flow network is graph which indicates additional possible flow. if there is path from source to sink in residual graph, then it is possible to add flow.

[BRANCH AND BOUND]
* generally used for solving combinatorial optimization problems.